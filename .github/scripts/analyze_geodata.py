import os
import json
import subprocess
import datetime
import shutil

WORKSPACE_DIR = "workspace"
OLD_STATS_FILE = "old_data/stats.json"
STATS_FILE = os.path.join(WORKSPACE_DIR, "stats.json")
README_FILE = os.path.join(WORKSPACE_DIR, "README.md")

def run_command(cmd):
    """è¿è¡Œç³»ç»Ÿå‘½ä»¤"""
    try:
        subprocess.check_call(cmd, shell=True, stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)
    except subprocess.CalledProcessError:
        print(f"âš ï¸ Warning: Command failed: {cmd}")

def count_lines(filepath):
    """è®¡ç®—æ–‡ä»¶è¡Œæ•°"""
    try:
        with open(filepath, 'rb') as f:
            return sum(1 for _ in f)
    except:
        return 0

def process_dat_files():
    """éå†ç›®å½•ï¼Œè§£åŒ… dat æ–‡ä»¶ï¼Œå¹¶è¿”å›ç»Ÿè®¡æ•°æ®"""
    current_stats = {}
    
    # éå† workspace ä¸‹çš„æ‰€æœ‰ä½œè€…ç›®å½•
    for author in os.listdir(WORKSPACE_DIR):
        author_path = os.path.join(WORKSPACE_DIR, author)
        if not os.path.isdir(author_path):
            continue
            
        print(f"ğŸ” Analyzing {author}...")
        current_stats[author] = {}

        # éå†ä½œè€…ç›®å½•ä¸‹çš„å­æ–‡ä»¶å¤¹ (geoip, geosite)
        for category in ["geoip", "geosite"]:
            cat_dir = os.path.join(author_path, category)
            if not os.path.exists(cat_dir):
                continue
                
            # æ‰¾åˆ°ç›®å½•ä¸‹çš„ .dat æ–‡ä»¶
            for file in os.listdir(cat_dir):
                if not file.endswith(".dat"):
                    continue
                
                dat_path = os.path.join(cat_dir, file)
                # åˆ›å»ºå¯¼å‡ºç›®å½•
                export_dir = os.path.join(cat_dir, f"{file}_text")
                if os.path.exists(export_dir):
                    shutil.rmtree(export_dir)
                os.makedirs(export_dir, exist_ok=True)
                
                print(f"  -> Extracting {file}...")
                
                # --- ä½¿ç”¨ v2dat è¿›è¡Œè§£åŒ… ---
                # è¯­æ³•: v2dat unpack geoip -o <output_dir> <dat_file>
                #       v2dat unpack geosite -o <output_dir> -f <filter_list> <dat_file>
                
                mode = "geoip" if "geoip" in file.lower() else "geosite"
                
                try:
                    # å°è¯•è§£åŒ…
                    # æ³¨æ„ï¼šv2dat unpack ä¼šæŠŠæ‰€æœ‰åˆ†ç±»è§£å‹æˆå•ç‹¬çš„æ–‡ä»¶åˆ°æŒ‡å®šç›®å½•
                    run_command(f"v2dat unpack {mode} -o {export_dir} {dat_path}")
                    
                    # ç»Ÿè®¡è§£åŒ…åçš„æ–‡ä»¶
                    if os.path.exists(export_dir):
                        files = os.listdir(export_dir)
                        # æŒ‘é€‰å‡ ä¸ªå…³é”®æ–‡ä»¶è¿›è¡Œç»Ÿè®¡ï¼Œé¿å…ç»Ÿè®¡å‡ åƒä¸ªæ–‡ä»¶
                        target_tags = ["cn", "google", "telegram", "private", "apple"]
                        
                        # å¦‚æœæ˜¯ geoipï¼Œç»Ÿè®¡ CN å’Œ US ç­‰
                        # å¦‚æœæ˜¯ geositeï¼Œç»Ÿè®¡ google, cn ç­‰
                        
                        for tag_file in files:
                            tag_name = os.path.splitext(tag_file)[0]
                            # åªç»Ÿè®¡æ„Ÿå…´è¶£çš„ Tagï¼Œæˆ–è€…ä½ å¯ä»¥å»æ‰è¿™ä¸ª if ç»Ÿè®¡æ‰€æœ‰
                            if tag_name in target_tags or len(files) < 20: 
                                full_path = os.path.join(export_dir, tag_file)
                                count = count_lines(full_path)
                                current_stats[author][f"{file}::{tag_name}"] = count
                                
                except Exception as e:
                    print(f"Failed to unpack {file}: {e}")

    return current_stats

def generate_markdown(current_stats, old_stats):
    """ç”Ÿæˆ README.md"""
    lines = ["# ğŸŒ GeoData Assets & Analytics", ""]
    lines.append(f"> Last Updated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} (UTC+8)")
    lines.append("")
    
    lines.append("## ğŸ“Š è§„åˆ™ç»Ÿè®¡ä¸å˜åŒ–")
    
    for author, rules in current_stats.items():
        if not rules: continue
        
        lines.append(f"### ğŸ‘¤ {author}")
        lines.append("| æ–‡ä»¶::æ ‡ç­¾ | æ¡ç›®æ•°é‡ | è¾ƒæ˜¨æ—¥å˜åŒ– |")
        lines.append("|---|---|---|")
        
        for key, count in sorted(rules.items()):
            # è®¡ç®— Diff
            old_count = old_stats.get(author, {}).get(key, 0)
            diff = count - old_count
            
            diff_str = "0"
            if diff > 0: diff_str = f"ğŸ”º +{diff}"
            elif diff < 0: diff_str = f"ğŸ”» {diff}"
            
            lines.append(f"| {key} | {count} | {diff_str} |")
        lines.append("")

    lines.append("## ğŸ“‚ ç›®å½•ç»“æ„è¯´æ˜")
    lines.append("- **geoip/**: äºŒè¿›åˆ¶ geoip.dat")
    lines.append("- **geosite/**: äºŒè¿›åˆ¶ geosite.dat")
    lines.append("- **xxx_text/**: è§£åŒ…åçš„æ–‡æœ¬è§„åˆ™ (æ–¹ä¾¿ Grep æˆ– è½¬æ¢)")
    
    with open(README_FILE, "w", encoding='utf-8') as f:
        f.write("\n".join(lines))
    
    # ä¿å­˜å½“å‰çš„ stats ä»¥å¤‡ä¸‹æ¬¡å¯¹æ¯”
    with open(STATS_FILE, "w", encoding='utf-8') as f:
        json.dump(current_stats, f, indent=2)

def main():
    print("â³ Loading old stats...")
    old_stats = {}
    if os.path.exists(OLD_STATS_FILE):
        try:
            with open(OLD_STATS_FILE, 'r') as f:
                old_stats = json.load(f)
        except:
            print("Old stats file corrupted, skipping diff.")

    print("â³ Processing assets...")
    current_stats = process_dat_files()
    
    print("â³ Generating report...")
    generate_markdown(current_stats, old_stats)
    print("âœ… Done.")

if __name__ == "__main__":
    main()
